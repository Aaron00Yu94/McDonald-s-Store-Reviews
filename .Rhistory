# Load necessary libraries
library(ggplot2)
library(dplyr)
library(patchwork)
library(scales)
library(viridis)
library(maps)
library(sf)
library(tidytext)
library(tidyr)
library(stringr)
# Set global chunk options
knitr::opts_chunk$set(
echo = TRUE,  # Show the R code in the output
message = FALSE,  # Suppress messages from library loading
warning = FALSE   # Suppress warnings in the output
)
# List of required packages
required_packages <- c(
"ggplot2", "dplyr", "patchwork", "scales", "viridis",
"maps", "sf", "tidytext", "tidyr", "stringr"
)
# Function to check and install missing packages
install_if_missing <- function(packages) {
for (pkg in packages) {
if (!requireNamespace(pkg, quietly = TRUE)) {
install.packages(pkg, dependencies = TRUE)
}
}
}
# Install and load required packages
install_if_missing(required_packages)
lapply(required_packages, library, character.only = TRUE)
# Load dataset from GitHub
github_url <- "https://raw.githubusercontent.com/Aaron00Yu94/McDonald-s-Store-Reviews/main/McDonald_s_Reviews.csv"
data <- read.csv(github_url)
# Data cleaning and processing
data <- data %>%
mutate(
avg_rating = as.numeric(gsub(" stars?", "", rating)),  # Convert rating to numeric
rating_count = as.numeric(gsub(",", "", rating_count))  # Remove commas and convert to numeric
) %>%
drop_na(longitude, latitude)  # Remove missing longitude and latitude
# Convert to sf object
sf_data <- data %>%
st_as_sf(coords = c("longitude", "latitude"), crs = 4326)
# Add scaled review count field
sf_data <- sf_data %>%
mutate(review_count_scaled = rescale(rating_count, to = c(1, 5)))
# Create a map of the United States using R's built-in map data
us_map <- map_data("state")
# Create the main map visualization
main_map <- ggplot() +
geom_polygon(data = us_map, aes(x = long, y = lat, group = group),
fill = "lightgray", color = "white") +
geom_point(
data = sf_data,
aes(x = st_coordinates(geometry)[, 1], y = st_coordinates(geometry)[, 2],
color = avg_rating, size = review_count_scaled),
alpha = 0.8
) +
scale_color_viridis_c(option = "plasma", name = "Avg Rating") +
scale_size_continuous(range = c(2, 8), name = "Review Count (scaled)") +
coord_fixed(1.3) +  # Keep aspect ratio fixed
labs(
title = "Geospatial Analysis of McDonald's Ratings and Review Counts",
x = "Longitude",
y = "Latitude"
) +
theme_minimal(base_size = 8) +
theme(
plot.title = element_text(hjust = 0.0, face = "bold", size = 12),
legend.position = "bottom",
legend.title = element_text(size = 8),
legend.text = element_text(size = 6)
)
# Create a bar chart showing review count distribution
review_dist <- ggplot(data, aes(x = avg_rating)) +
geom_bar(fill = "skyblue", color = "black") +
labs(title = "Distribution of Ratings", x = "Average Rating", y = "Count") +
theme_minimal(base_size = 8) +
theme(
plot.title = element_text(hjust = 0.1, size = 10),
axis.title = element_text(size = 8),
axis.text = element_text(size = 8)
)
# Combine the map and distribution plots
combined_plot <- main_map / review_dist +
plot_layout(heights = c(5, 1))  # Map height is 3x bar chart height
# Display the combined visualization
combined_plot
# Data cleaning: remove non-alphanumeric characters and extra spaces
processed_data <- data %>%
mutate(review_clean = str_replace_all(review, "[^[:alnum:][:space:]!?.]", "")) %>%  # Remove non-alphanumeric characters
mutate(review_clean = str_squish(review_clean))  # Remove extra spaces
# Tokenize words from the cleaned reviews
data_tokens <- processed_data %>%
unnest_tokens(word, review_clean)
# Load Bing sentiment lexicon
sentiment_lexicon <- get_sentiments("bing")
# Calculate sentiment scores for each review
data_sentiment <- data_tokens %>%
inner_join(sentiment_lexicon, by = "word") %>%
group_by(review) %>%
summarise(sentiment_score = sum(sentiment == "positive") - sum(sentiment == "negative"))
# Merge sentiment scores back into the original data
processed_data <- processed_data %>%
left_join(data_sentiment, by = "review") %>%
mutate(sentiment_score = ifelse(is.na(sentiment_score), 0, sentiment_score))  # Assign 0 if no matches in sentiment lexicon
# ------------------------------
# PCA Analysis Section
# ------------------------------
# Select relevant features and remove missing values
pca_data <- processed_data %>%
select(avg_rating, rating_count, sentiment_score) %>%  # Assumes avg_rating and rating_count columns exist
drop_na()  # Remove rows with missing values
# Standardize the data
scaled_pca_data <- scale(pca_data)
# Perform PCA
pca_result <- prcomp(scaled_pca_data, center = TRUE, scale. = TRUE)
# Display variance explained by principal components
summary(pca_result)
# Convert PCA scores to a data frame
pca_scores <- as.data.frame(pca_result$x)
# Add original ratings as a color coding variable
pca_scores <- cbind(pca_scores, avg_rating = pca_data$avg_rating)
# ------------------------------
# PCA Visualization
# ------------------------------
# Plot PCA scores for the first two principal components
pca_plot <- ggplot(pca_scores, aes(x = PC1, y = PC2, color = avg_rating)) +
geom_point(alpha = 0.8, size = 2) +
scale_color_viridis_c(name = "Average Rating") +
labs(
title = "PCA Scores: Exploring Relationships Between Sentiment, Ratings, and Reviews",
x = "Principal Component 1",
y = "Principal Component 2"
) +
theme_minimal()
# Display PCA scores plot
print(pca_plot)
# ------------------------------
# PCA Loadings Visualization
# ------------------------------
# Extract the loadings matrix
loading_matrix <- as.data.frame(pca_result$rotation)
# Plot the loadings for the first two principal components
loading_plot <- ggplot(loading_matrix, aes(x = PC1, y = PC2, label = rownames(loading_matrix))) +
geom_point(color = "blue", size = 3) +
geom_text(vjust = 1.5, hjust = 1.5) +
labs(
title = "PCA Loadings: Understanding Ratings and Sentiment Through PCA Loadings",
x = "Principal Component 1",
y = "Principal Component 2"
) +
theme_minimal()
# Display PCA loadings plot
print(loading_plot)
# List of required packages
required_packages <- c(
"ggplot2", "dplyr", "patchwork", "scales", "viridis",
"maps", "sf", "tidytext", "tidyr", "stringr"
)
# Function to check and install missing packages
install_if_missing <- function(packages) {
for (pkg in packages) {
if (!requireNamespace(pkg, quietly = TRUE)) {
install.packages(pkg, dependencies = TRUE)
}
}
}
# Install and load required packages
install_if_missing(required_packages)
lapply(required_packages, library, character.only = TRUE)
# Set global chunk options
knitr::opts_chunk$set(
echo = TRUE,  # Show the R code in the output
message = FALSE,  # Suppress messages from library loading
warning = FALSE   # Suppress warnings in the output
)
