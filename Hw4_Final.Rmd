---
title: "Geospatial Analysis of McDonald's Ratings"
author: "Aaron Yu"
output: html_document
---

```{r setup, include=FALSE}
# List of required packages
required_packages <- c(
  "ggplot2", "dplyr", "patchwork", "scales", "viridis", 
  "maps", "sf", "tidytext", "tidyr", "stringr"
)

# Function to check and install missing packages
install_if_missing <- function(packages) {
  for (pkg in packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
      install.packages(pkg, dependencies = TRUE)
    }
  }
}

# Install and load required packages
install_if_missing(required_packages)
lapply(required_packages, library, character.only = TRUE)

# Set global chunk options
knitr::opts_chunk$set(
  echo = TRUE,  # Show the R code in the output
  message = FALSE,  # Suppress messages from library loading
  warning = FALSE   # Suppress warnings in the output
)
```

```{r}
# Load dataset from GitHub
github_url <- "https://raw.githubusercontent.com/Aaron00Yu94/McDonald-s-Store-Reviews/main/McDonald_s_Reviews.csv"
data <- read.csv(github_url)

# Data cleaning and processing
data <- data %>%
  mutate(
    avg_rating = as.numeric(gsub(" stars?", "", rating)),  # Convert rating to numeric
    rating_count = as.numeric(gsub(",", "", rating_count))  # Remove commas and convert to numeric
  ) %>%
  drop_na(longitude, latitude)  # Remove missing longitude and latitude

# Convert to sf object
sf_data <- data %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)

# Add scaled review count field
sf_data <- sf_data %>%
  mutate(review_count_scaled = rescale(rating_count, to = c(1, 5)))

```


```{r}
# Create a map of the United States using R's built-in map data
us_map <- map_data("state")

# Create the main map visualization
main_map <- ggplot() +
  geom_polygon(data = us_map, aes(x = long, y = lat, group = group),
               fill = "lightgray", color = "white") +
  geom_point(
    data = sf_data,
    aes(x = st_coordinates(geometry)[, 1], y = st_coordinates(geometry)[, 2], 
        color = avg_rating, size = review_count_scaled),
    alpha = 0.8
  ) +
  scale_color_viridis_c(option = "plasma", name = "Avg Rating") +
  scale_size_continuous(range = c(2, 8), name = "Review Count (scaled)") +
  coord_fixed(1.3) +  # Keep aspect ratio fixed
  labs(
    title = "Geospatial Analysis of McDonald's Ratings and Review Counts",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal(base_size = 8) +
  theme(
    plot.title = element_text(hjust = 0.0, face = "bold", size = 12),
    legend.position = "bottom",
    legend.title = element_text(size = 8),
    legend.text = element_text(size = 6)
  )

```

```{r}
# Create a bar chart showing review count distribution
review_dist <- ggplot(data, aes(x = avg_rating)) +
  geom_bar(fill = "skyblue", color = "black") +
  labs(title = "Distribution of Ratings", x = "Average Rating", y = "Count") +
  theme_minimal(base_size = 8) +
  theme(
    plot.title = element_text(hjust = 0.1, size = 10),
    axis.title = element_text(size = 8),
    axis.text = element_text(size = 8)
  )

```

```{r}
# Combine the map and distribution plots
combined_plot <- main_map / review_dist +
  plot_layout(heights = c(5, 1))  # Map height is 3x bar chart height

# Display the combined visualization
combined_plot
```

---
title: "McDonald's Ratings and Reviews Analysis"
output: html_document
---

## Discussion of Visualizations

### **1. What is the essential question that your visualization is supposed to inform?**
1. How are McDonald’s store ratings and review counts distributed geographically?  
2. Are extreme ratings (1 or 5 stars) more common than moderate ratings (2, 3, or 4 stars)?  
3. How do geographical patterns relate to the overall rating distribution?

---

### **2. How do aspects of your design support exploration of the essential question?**
- **Design Choices:**
  - The **map** uses colors to represent average ratings and point sizes for review counts, helping visualize geographical trends.
  - The **bar chart** summarizes the overall rating distribution, showing a higher frequency of 1-star and 5-star ratings.
- **Trade-offs:**
  - The map focuses on ratings and review counts, omitting other metrics like sentiment score to keep it clear.
  - The bar chart doesn’t differentiate regions, prioritizing a simple overview of rating distribution.

---

### **3. What are your key findings? How do they relate to your prior understanding?**
- **Key Findings:**
  - High-rating stores (yellow) cluster in specific regions, while low ratings (purple) are dispersed, indicating regional performance differences.
  - Extreme ratings (1 and 5 stars) dominate, while moderate ratings are rare, showing polarized customer feedback.
  - Combining the map and bar chart highlights the interplay between regional patterns and overall trends.
- **Relation to Prior Understanding:**  
  These findings align with the expectation of polarized reviews but reveal new insights, like regional clustering of high ratings.

---

### **4. How did you create the visualizations? Were there any data preparation steps?**
1. **Data Preparation:**
   - Converted ratings to numeric values and dropped missing geographic data.
   - Scaled review counts for proportional visualization and created an `sf` object for mapping.
2. **Visualization:**
   - The **map** displays geographic trends using `ggplot2`, with `avg_rating` mapped to color and scaled `rating_count` to size.
   - The **bar chart** summarizes rating frequencies.
   - Both visualizations were combined using the `patchwork` package.

---



```{r}
# Data cleaning: remove non-alphanumeric characters and extra spaces
processed_data <- data %>%
  mutate(review_clean = str_replace_all(review, "[^[:alnum:][:space:]!?.]", "")) %>%  # Remove non-alphanumeric characters
  mutate(review_clean = str_squish(review_clean))  # Remove extra spaces

# Tokenize words from the cleaned reviews
data_tokens <- processed_data %>%
  unnest_tokens(word, review_clean)

# Load Bing sentiment lexicon
sentiment_lexicon <- get_sentiments("bing")

# Calculate sentiment scores for each review
data_sentiment <- data_tokens %>%
  inner_join(sentiment_lexicon, by = "word") %>%
  group_by(review) %>%
  summarise(sentiment_score = sum(sentiment == "positive") - sum(sentiment == "negative"))

# Merge sentiment scores back into the original data
processed_data <- processed_data %>%
  left_join(data_sentiment, by = "review") %>%
  mutate(sentiment_score = ifelse(is.na(sentiment_score), 0, sentiment_score))  # Assign 0 if no matches in sentiment lexicon

# ------------------------------
# PCA Analysis Section
# ------------------------------

# Select relevant features and remove missing values
pca_data <- processed_data %>%
  select(avg_rating, rating_count, sentiment_score) %>%  # Assumes avg_rating and rating_count columns exist
  drop_na()  # Remove rows with missing values

# Standardize the data
scaled_pca_data <- scale(pca_data)

# Perform PCA
pca_result <- prcomp(scaled_pca_data, center = TRUE, scale. = TRUE)

# Display variance explained by principal components
summary(pca_result)

# Convert PCA scores to a data frame
pca_scores <- as.data.frame(pca_result$x)

# Add original ratings as a color coding variable
pca_scores <- cbind(pca_scores, avg_rating = pca_data$avg_rating)

# ------------------------------
# PCA Visualization
# ------------------------------

# Plot PCA scores for the first two principal components
pca_plot <- ggplot(pca_scores, aes(x = PC1, y = PC2, color = avg_rating)) +
  geom_point(alpha = 0.8, size = 2) +
  scale_color_viridis_c(name = "Average Rating") +
  labs(
    title = "PCA Scores: Exploring Relationships Between Sentiment, Ratings, and Reviews",
    x = "Principal Component 1",
    y = "Principal Component 2"
  ) +
  theme_minimal()

# Display PCA scores plot
print(pca_plot)

# ------------------------------
# PCA Loadings Visualization
# ------------------------------

# Extract the loadings matrix
loading_matrix <- as.data.frame(pca_result$rotation)

# Plot the loadings for the first two principal components
loading_plot <- ggplot(loading_matrix, aes(x = PC1, y = PC2, label = rownames(loading_matrix))) +
  geom_point(color = "blue", size = 3) +
  geom_text(vjust = 1.5, hjust = 1.5) +
  labs(
    title = "PCA Loadings: Understanding Ratings and Sentiment Through PCA Loadings",
    x = "Principal Component 1",
    y = "Principal Component 2"
  ) +
  theme_minimal()

# Display PCA loadings plot
print(loading_plot)

```

---
title: "PCA Analysis of McDonald's Reviews"
output: html_document
---

## Discussion of Visualizations

### **1. What is the essential question that your visualization is supposed to inform?**
- **PCA Scores Plot:** What are the patterns in the relationship between ratings, sentiment scores, and review counts? Are there clusters or trends?  
- **PCA Loadings Plot:** How do ratings, sentiment, and review counts contribute to the principal components?  
- **Variance Table:** How much variance is captured by the principal components, and is focusing on the first two sufficient?

---

### **2. How do aspects of your design support exploration of the essential question?**

- **PCA Scores Plot:**
  - **Design:** Color encodes `avg_rating`, and PC1/PC2 axes show relationships across 78.79% of variance.
  - **Trade-offs:** Higher components are ignored, and review counts are indirectly represented.

- **PCA Loadings Plot:**
  - **Design:** Variable positions and labels show contributions to PC1/PC2.
  - **Trade-offs:** Focuses only on the first two components, excluding others.

- **Variance Table:**
  - **Design:** Summarizes explained variance to justify dimensionality reduction.
  - **Trade-offs:** A cumulative variance plot could better visualize trends.

---

### **3. What are your key findings? How do they relate to your prior understanding?**

1. **PCA Scores Plot:**
   - PC1 (46.78%) is strongly linked to `avg_rating`, while PC2 (32.01%) captures `sentiment_score`.
   - High ratings cluster, while low ratings are more dispersed.

2. **PCA Loadings Plot:**
   - `avg_rating` heavily contributes to PC1; `sentiment_score` drives PC2.
   - `rating_count` has minimal influence on both components.

3. **Variance Table:**
   - The first two components explain 78.79% of the variance, validating the focus on them.

**Relation to Prior Understanding:** These findings confirm that ratings and sentiment are dominant factors but reveal new nuances, such as variability in low ratings.

---

### **4. How did you create the visualizations? Were there any data preparation steps?**

1. **Data Preparation:**
   - Cleaned text reviews, calculated sentiment scores using the Bing lexicon, and replaced missing values with 0.
   - Selected `avg_rating`, `rating_count`, and `sentiment_score` for PCA and standardized them using `scale()`.

2. **Visualization Creation:**
   - **PCA Scores Plot:** Scatter plot of PC1 vs. PC2, colored by `avg_rating` using `ggplot2`.
   - **PCA Loadings Plot:** Loadings matrix visualized with variable labels to explain PC contributions.
   - **Variance Table:** Summarized the variance captured by components.

---

### **Summary**
The PCA analysis uncovers relationships between ratings, sentiment, and review counts. PCA Scores Plot shows trends and clustering, while PCA Loadings Plot reveals variable contributions. Together, they effectively highlight the dataset's primary patterns, confirming that ratings and sentiment dominate customer feedback analysis.

